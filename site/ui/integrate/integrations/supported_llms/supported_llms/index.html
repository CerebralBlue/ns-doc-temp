
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Explore NeuralSeek's extensive support for LLMs from top providers like Amazon, Azure, Google, and OpenAI. Discover how to integrate and configure these models for seamless AI experiences.">
      
      
      
        <link rel="canonical" href="https://documentation.neuralseek.com/ui/integrate/integrations/supported_llms/supported_llms/">
      
      
        <link rel="prev" href="../../supported_knowledgebases/supported_knowledgebases/">
      
      
        <link rel="next" href="../../supported_virtual_agents/supported_virtual_agents/">
      
      
      <link rel="icon" href="../../../../../assets/ns-new.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Supported LLMs - NeuralSeek Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
    
      
    
    <script type="module">
    import { NsChat } from 'https://stagingapi.neuralseek.com/src/chatSDK.js';

    const chatConfig = {
      "userId": "",
      "chatElement": "chat",
      "chatHistoryElement": "chathistory",
      "chatOverlayToggleButtonElement": "chat-toggle-btn",
      "enableChatHistory": false,
      "enableChatOverlayToggleButton": true,
      "chatOverlayHeaderTitle": "NeuralSeek",
      "includeUrlInResponse": true,
      "urlDisplayText": "See more here",
      "apiServer": "https://stagingapi.neuralseek.com",
      "loadingAnimationURL": "https://stagingapi.neuralseek.com/images/ns-loader-chat.svg",
      "chatTimeout": 23000,
      "instanceId": "NS-ES-V2",
      "embedCode": 656054868,
      "streaming": true,
      "maistroLed": false,
      "maistroFlow": "",
      "enableDrop": true,
      "allowedFiles": [
        ".png",
        ".jpg",
        ".jpeg"
      ],
      "welcomeMessage": "Welcome to NeuralSeek!",
      "welcomeBotMessages": [
        "How can we help?"
      ],
      "welcomeButtons": [
        "Tell me about NeuralSeek"
      ],
      "turnHistoryLimit": 1,
      "includeRequired": true
    }
    const chat = new NsChat(chatConfig);
    </script>


    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../../../../stylesheets/chatbot.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-LE5XX6X6Z7"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-LE5XX6X6Z7",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-LE5XX6X6Z7",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Supported LLMs - NeuralSeek Documentation" >
      
        <meta  property="og:description"  content="Explore NeuralSeek's extensive support for LLMs from top providers like Amazon, Azure, Google, and OpenAI. Discover how to integrate and configure these models for seamless AI experiences." >
      
        <meta  property="og:image"  content="https://documentation.neuralseek.com/assets/images/social/ui/integrate/integrations/supported_llms/supported_llms.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://documentation.neuralseek.com/ui/integrate/integrations/supported_llms/supported_llms/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Supported LLMs - NeuralSeek Documentation" >
      
        <meta  name="twitter:description"  content="Explore NeuralSeek's extensive support for LLMs from top providers like Amazon, Azure, Google, and OpenAI. Discover how to integrate and configure these models for seamless AI experiences." >
      
        <meta  name="twitter:image"  content="https://documentation.neuralseek.com/assets/images/social/ui/integrate/integrations/supported_llms/supported_llms.png" >
      
    
    
   <link href="../../../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#overview" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="NeuralSeek Documentation" class="md-header__button md-logo" aria-label="NeuralSeek Documentation" data-md-component="logo">
      
  <img src="../../../../../assets/ns-new.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            NeuralSeek Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Supported LLMs
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="NeuralSeek Documentation" class="md-nav__button md-logo" aria-label="NeuralSeek Documentation" data-md-component="logo">
      
  <img src="../../../../../assets/ns-new.png" alt="logo">

    </a>
    NeuralSeek Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Explore the Interface
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Explore the Interface
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../home/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../configure/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Configure
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Integrate
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Integrate
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2_4_2" id="__nav_2_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Platform Integrations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_4_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_4_2">
            <span class="md-nav__icon md-icon"></span>
            Platform Integrations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supported_knowledgebases/supported_knowledgebases/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KnowledgeBases
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Supported LLMs
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Supported LLMs
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configuring_an_llm" class="md-nav__link">
    <span class="md-ellipsis">
      Configuring an LLM
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supported_virtual_agents/supported_virtual_agents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supported Virtual Agents
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../extract/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Extract
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../load/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../maistro/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    mAIstro
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_7" id="__nav_2_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7">
            <span class="md-nav__icon md-icon"></span>
            mAIstro
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_2" >
        
          
          <label class="md-nav__link" for="__nav_2_7_2" id="__nav_2_7_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    NTL Functions
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7_2">
            <span class="md-nav__icon md-icon"></span>
            NTL Functions
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/get_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Get Data
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/upload_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Upload Data
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/generate_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generate Data
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/local_cache/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Local Cache
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/extract_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Extract Data
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/multi-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multi-Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/control_flow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Control Flow
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/rag_tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG Tools
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/guardrails/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GuardRails
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/system_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    System Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/sandboxes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sandboxes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_2_12" >
        
          
          <label class="md-nav__link" for="__nav_2_7_2_12" id="__nav_2_7_2_12_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Modify Data
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_2_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7_2_12">
            <span class="md-nav__icon md-icon"></span>
            Modify Data
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/modify_data/code_toolbox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Code Toolbox
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/modify_data/json_toolbox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    JSON Toolbox
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/modify_data/string_toolbox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    String Toolbox
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/modify_data/transform/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transform
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/modify_data/xml_toolbox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    XML Toolbox
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/send_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Send Data
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_2_14" >
        
          
          <label class="md-nav__link" for="__nav_2_7_2_14" id="__nav_2_7_2_14_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Integrations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_2_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7_2_14">
            <span class="md-nav__icon md-icon"></span>
            Integrations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/aws_s3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AWS S3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/databases/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Databases
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/github/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Github
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/google_drive/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Google Drive
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/jira/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jira
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/knowledgebases/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KnowledgeBases
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/sharepoint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sharepoint
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/slack/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Slack
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/trello/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Trello
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/watsonx.governance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    watsonx.governance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../maistro/features/ntl_functions/integrations/web_searches/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Web Searches
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../seek/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Seek
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../chat/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chat
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../curate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Curate
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../governance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Governance
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Guides & Walkthroughs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Guides & Walkthroughs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Data
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Data
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/data/proposals/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Proposals
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1">
            <span class="md-nav__icon md-icon"></span>
            Proposals
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/data/tuning_guide/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Tuning Guide
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_2">
            <span class="md-nav__icon md-icon"></span>
            Tuning Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/data/dynamic_filters/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Dynamic Filters
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_3">
            <span class="md-nav__icon md-icon"></span>
            Dynamic Filters
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/data/virtual_kb/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    VirtualKB
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_4">
            <span class="md-nav__icon md-icon"></span>
            VirtualKB
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_5" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/data/doc_ingestion/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Document Ingestion
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_5">
            <span class="md-nav__icon md-icon"></span>
            Document Ingestion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_6" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/data/replay_guide/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Replay
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_6">
            <span class="md-nav__icon md-icon"></span>
            Replay
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Integration
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Integration
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_1" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/integration/chat_sdk_integration/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Chat SDK
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_1">
            <span class="md-nav__icon md-icon"></span>
            Chat SDK
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/integration/training_virtual_agents/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Training Virtual Agents
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_2">
            <span class="md-nav__icon md-icon"></span>
            Training Virtual Agents
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/integration/backup_and_restore/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Backup and Restore
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_3">
            <span class="md-nav__icon md-icon"></span>
            Backup and Restore
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/integration/elasticsearch_vector_model/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    ElasticSearch Vector Model
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_4">
            <span class="md-nav__icon md-icon"></span>
            ElasticSearch Vector Model
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_5" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/integration/pinecone_configuration/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Pinecone Configuration
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_5">
            <span class="md-nav__icon md-icon"></span>
            Pinecone Configuration
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_6" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/integration/providing_context/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Passing Conversational Context
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_6">
            <span class="md-nav__icon md-icon"></span>
            Passing Conversational Context
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_7" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/integration/wa_context_guide/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    mAIstro Streaming Endpoint with Watsonx
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_7">
            <span class="md-nav__icon md-icon"></span>
            mAIstro Streaming Endpoint with Watsonx
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_8" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/integration/implementing_feedback/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Implementing Feedback
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_8">
            <span class="md-nav__icon md-icon"></span>
            Implementing Feedback
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_9" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/integration/nice_cxone_integration/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    NICE CXone
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_9">
            <span class="md-nav__icon md-icon"></span>
            NICE CXone
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_1" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/models/multimodal/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Multimodal LLM Configuration
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_1">
            <span class="md-nav__icon md-icon"></span>
            Multimodal LLM Configuration
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../guides/models/semantic_model/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Semantic Model Tuning
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_2">
            <span class="md-nav__icon md-icon"></span>
            Semantic Model Tuning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../more_about_NS/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    More About NeuralSeek
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            More About NeuralSeek
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../more_about_NS/partners/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    NeuralSeek Partnerships
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../more_about_NS/plans/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Available NeuralSeek Plans
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../more_about_NS/data_security_and_privacy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Security and Privacy
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../changelog/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Changelog
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configuring_an_llm" class="md-nav__link">
    <span class="md-ellipsis">
      Configuring an LLM
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
    
                  

  <nav class="md-tags" >
    
      
      
      
        <span class="md-tag">supportedintegration</span>
      
    
  </nav>



  <h1>Supported LLMs</h1>

<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link"></a></h2>
<p>NeuralSeek supports LLMs from many providers, including:</p>
<ul>
<li>Amazon Bedrock</li>
<li>Azure Cognitive Services</li>
<li>Cloudflare</li>
<li>Google Vertex AI</li>
<li>HuggingFace</li>
<li>OpenAI</li>
<li>together.ai</li>
<li>watsonx.ai</li>
</ul>
<p>In addition to any generic OpenAI-compatible endpoints.</p>
<p><br/></p>
<p><strong>Supported LLM details by provider:</strong></p>
<details class="success">
<summary>Amazon Bedrock</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude 3 Haiku</td>
<td>Claude 3 Haiku is Anthropic's fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with speed. Customers will be able to build seamless AI experiences that mimic human interactions. Claude 3 Haiku can process images and return text outputs, and features a 200K context window.</td>
</tr>
<tr>
<td>Claude 3 Opus</td>
<td>Claude 3 Opus is Anthropic's most powerful AI model, with state-of-the-art performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Claude 3 Opus shows us the frontier of what’s possible with generative AI. Claude 3 Opus can process images and return text outputs, and features a 200K context window.</td>
</tr>
<tr>
<td>Claude 3 Sonnet</td>
<td>Claude 3 Sonnet by Anthropic strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It offers maximum utility at a lower price than competitors, and is engineered to be the dependable, high-endurance workhorse for scaled AI deployments. Claude 3 Sonnet can process images and return text outputs, and features a 200K context window.</td>
</tr>
<tr>
<td>Claude 3.5 Haiku</td>
<td>Claude 3.5 Haiku is Anthropic's fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with speed. Customers will be able to build seamless AI experiences that mimic human interactions. Claude 3 Haiku can process images and return text outputs, and features a 200K context window.</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet</td>
<td>Claude 3.5 Sonnet by Anthropic strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It offers maximum utility at a lower price than competitors, and is engineered to be the dependable, high-endurance workhorse for scaled AI deployments. Claude 3 Sonnet can process images and return text outputs, and features a 200K context window.</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet v2</td>
<td>Claude 3.5 Sonnet v2 by Anthropic strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It offers maximum utility at a lower price than competitors, and is engineered to be the dependable, high-endurance workhorse for scaled AI deployments. Claude 3 Sonnet can process images and return text outputs, and features a 200K context window.</td>
</tr>
<tr>
<td>Claude Instant v1.2</td>
<td>A faster and cheaper yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document question-answering.</td>
</tr>
<tr>
<td>Claude v2</td>
<td>Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following.</td>
</tr>
<tr>
<td>Claude v2.1</td>
<td>Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following.</td>
</tr>
<tr>
<td>Jurassic-2 Mid</td>
<td>Jurassic-2 Mid is AI21’s mid-sized model, carefully designed to strike the right balance between exceptional quality and affordability. Jurassic-2 Mid can be applied to any language comprehension or generation task including question answering, summarization, long-form copy generation, advanced information extraction and many others.</td>
</tr>
<tr>
<td>Jurassic-2 Ultra</td>
<td>Jurassic-2 Ultra is AI21’s most powerful model offering exceptional quality. Apply Jurassic-2 Ultra to complex tasks that require advanced text generation and comprehension. Popular use cases include question answering, summarization, long-form copy generation, advanced information extraction, and more.</td>
</tr>
<tr>
<td>Llama-2-chat 13B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Llama-2-chat 70B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>llama-3-1-405b-instruct</td>
<td>llama-3-405b instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-1-70b-instruct</td>
<td>Llama 3.1 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-1-8b-instruct</td>
<td>Llama 3.1 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-2-11b-vision-instruct</td>
<td>Llama 3.2 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-2-1b-instruct</td>
<td>Llama 3.2 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-2-3b-instruct</td>
<td>Llama 3.2 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-2-90b-vision-instruct</td>
<td>Llama 3.2 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-3-70b-instruct</td>
<td>Llama 3.3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mistral-large</td>
<td>The most advanced Mistral AI Large Language model capable of handling any language task including complex multilingual reasoning, text understanding, transformation, and code generation.</td>
</tr>
<tr>
<td>Mistral-small</td>
<td>Mistraql Small is optimized for high-volume, low-latency language-based tasks. Mistral Small is perfectly suited for straightforward tasks that can be performed in bulk, such as classification, customer support, or text generation.</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct</td>
<td>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Nova Lite</td>
<td>Amazon Nova is a new generation of state-of-the-art (SOTA) foundation models (FMs) that deliver frontier intelligence and industry leading price-performance, available exclusively on Amazon Bedrock.</td>
</tr>
<tr>
<td>Nova Micro</td>
<td>Amazon Nova is a new generation of state-of-the-art (SOTA) foundation models (FMs) that deliver frontier intelligence and industry leading price-performance, available exclusively on Amazon Bedrock.</td>
</tr>
<tr>
<td>Nova Pro</td>
<td>Amazon Nova is a new generation of state-of-the-art (SOTA) foundation models (FMs) that deliver frontier intelligence and industry leading price-performance, available exclusively on Amazon Bedrock.</td>
</tr>
<tr>
<td>Titan Text G1 - Express</td>
<td>Amazon Titan Text Express has a context length of up to 8,000 tokens, making it well-suited for a wide range of advanced, general language tasks such as open-ended text generation and conversational chat, as well as support within Retrieval Augmented Generation (RAG). At launch, the model is optimized for English, with multilingual support for more than 100 additional languages available in preview.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>Azure Cognitive Services</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Azure GPT4 Turbo (Preview)</td>
<td>GPT-4 Turbo provides a good balance of speed and capability.  The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>GPT-4o matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages.</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>GPT-4o mini is an affordable and intelligent small model for fast, lightweight tasks. GPT-4o mini is cheaper and more capable than GPT-3.5 Turbo..</td>
</tr>
<tr>
<td>GPT3.5</td>
<td>GPT-3.5 provides a good balance of speed and capability.</td>
</tr>
<tr>
<td>GPT4</td>
<td>GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout.</td>
</tr>
<tr>
<td>GPT4 (32K)</td>
<td>GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 32K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
<tr>
<td>o1</td>
<td>The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</td>
</tr>
<tr>
<td>o1-mini</td>
<td>The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</td>
</tr>
<tr>
<td>o1-preview</td>
<td>The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>Cloudflare</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-chat-7b</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>llama-3-8b-instruct</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3.1-8b-instruct</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3.2-1b-instruct</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3.2-3b-instruct</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>m2m100-1.2b</td>
<td>M2M100 is a multilingual encoder-decoder (seq-to-seq) model primarily intended for translation tasks. M2M100 does not support custom translation dictionaries.</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>Google Vertex AI</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>gemini-1.5-flash (128K Context)</td>
<td>Gemini 1.5 Flash is designed for high-volume, high-frequency tasks where cost and latency matter. On most common tasks, Flash achieves comparable quality to other Gemini Pro models at a significantly reduced cost. Flash is well-suited for applications like chat assistants and on-demand content generation where speed and scale matter.</td>
</tr>
<tr>
<td>gemini-1.5-flash (1M Context)</td>
<td>Gemini 1.5 Flash is designed for high-volume, high-frequency tasks where cost and latency matter. On most common tasks, Flash achieves comparable quality to other Gemini Pro models at a significantly reduced cost. Flash is well-suited for applications like chat assistants and on-demand content generation where speed and scale matter.</td>
</tr>
<tr>
<td>gemini-1.5-flash-001 (128K Context)</td>
<td>Gemini 1.5 Flash is designed for high-volume, high-frequency tasks where cost and latency matter. On most common tasks, Flash achieves comparable quality to other Gemini Pro models at a significantly reduced cost. Flash is well-suited for applications like chat assistants and on-demand content generation where speed and scale matter.</td>
</tr>
<tr>
<td>gemini-1.5-flash-001 (1M Context)</td>
<td>Gemini 1.5 Flash is designed for high-volume, high-frequency tasks where cost and latency matter. On most common tasks, Flash achieves comparable quality to other Gemini Pro models at a significantly reduced cost. Flash is well-suited for applications like chat assistants and on-demand content generation where speed and scale matter.</td>
</tr>
<tr>
<td>gemini-1.5-flash-8b</td>
<td>Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. .</td>
</tr>
<tr>
<td>gemini-1.5-pro (128K Context)</td>
<td>Gemini 1.5 Pro is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.</td>
</tr>
<tr>
<td>gemini-1.5-pro (1M Context)</td>
<td>Gemini 1.5 Pro is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.</td>
</tr>
<tr>
<td>gemini-1.5-pro-001 (128K Context)</td>
<td>Gemini 1.5 Pro is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.</td>
</tr>
<tr>
<td>gemini-1.5-pro-001 (1M Context)</td>
<td>Gemini 1.5 Pro is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.</td>
</tr>
<tr>
<td>gemini-2.0-flash-001</td>
<td>Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window.</td>
</tr>
<tr>
<td>gemini-2.0-flash-exp</td>
<td>Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window.</td>
</tr>
<tr>
<td>gemini-2.0-flash-lite-preview-02-05</td>
<td>Gemini 2.0 Flash-Lite is the fastest and most cost efficient Flash model. It's an upgrade path for 1.5 Flash users who want better quality for the same price and speed.</td>
</tr>
<tr>
<td>gemini-2.0-pro-exp-02-05</td>
<td>Gemini 2.0 Pro is the strongest model for coding and world knowledge and features a 2M long context window. Gemini 2.0 Pro is available as an experimental model in Vertex AI and is an upgrade path for 1.5 Pro users who want better quality, or who are particularly invested in long context and code.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>HuggingFace</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Flan-t5-xxl</td>
<td>The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</td>
</tr>
<tr>
<td>Flan-ul2</td>
<td>The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</td>
</tr>
<tr>
<td>Llama-2</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf)</td>
</tr>
<tr>
<td>Llama-2-chat</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>llama-3-chat</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mixtral-8x22B-Instruct-v0.1</td>
<td>The Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. It outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct</td>
<td>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>MPT-7B-instruct</td>
<td>The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses.  Be sure to set a minimum confidence level to control this. Not reccomended for public usecases.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>NeuralSeek</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>llama-3.1-8b-instruct</td>
<td>Llama 3.1 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version. This is a Globally-hosted model and is not guaranteed to run within a particular geography. Do not use this model if you have data residency requirements.</td>
</tr>
<tr>
<td>Translate</td>
<td>The translate model is an encoder-decoder model focused on language translation. This model does not support custom translation dictionaries or additional LLM instructions. If you require those capabilities do not use this model.This is a Globally-hosted model and is not guaranteed to run within a particular geography. Do not use this model if you have data residency requirements.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>OpenAI</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt-3.5-turbo-0125</td>
<td>GPT-3.5 provides a good balance of speed and capability.</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>GPT-4o matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages.</td>
</tr>
<tr>
<td>gpt-4o-mini</td>
<td>GPT-4o mini is an affordable and intelligent small model for fast, lightweight tasks. GPT-4o mini is cheaper and more capable than GPT-3.5 Turbo..</td>
</tr>
<tr>
<td>GPT3.5</td>
<td>GPT-3.5 provides a good balance of speed and capability.</td>
</tr>
<tr>
<td>GPT3.5 (16K)</td>
<td>GPT-3.5 provides a good balance of speed and capability.  The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
<tr>
<td>GPT4</td>
<td>GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout.</td>
</tr>
<tr>
<td>GPT4 (32K)</td>
<td>GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
<tr>
<td>GPT4 Turbo (Preview)</td>
<td>GPT-4 Turbo provides a good balance of speed and capability.  The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
<tr>
<td>o1</td>
<td>The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</td>
</tr>
<tr>
<td>o1-mini</td>
<td>The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</td>
</tr>
<tr>
<td>o1-preview</td>
<td>The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</td>
</tr>
<tr>
<td>o3-mini</td>
<td>The o3 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>together.ai</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>DeepSeek-R1</td>
<td>DeepSeek-R1, a strong Mixture-of-Experts (MoE) language model with 671B total parameters .</td>
</tr>
<tr>
<td>DeepSeek-V3</td>
<td>DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters .</td>
</tr>
<tr>
<td>Llama-2 Chat 13B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Llama-2 Chat 70B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Llama-2 Chat 7B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>llama-2-13b</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf)</td>
</tr>
<tr>
<td>llama-2-70b</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf)</td>
</tr>
<tr>
<td>LLaMA-2-7B-32K-Instruct</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf)</td>
</tr>
<tr>
<td>Llama-3.1-405B-Instruct-Turbo</td>
<td>llama-3-405b instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks.</td>
</tr>
<tr>
<td>Llama-3.1-8B-Instruct-Turbo-128K</td>
<td>Llama-3.1-8B-Instruct-Turbo-128K instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks.</td>
</tr>
<tr>
<td>Llama-3.3-70B-Instruct-Turbo</td>
<td>Llama-3.3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks.</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mixtral-8x22B-Instruct-v0.1</td>
<td>The Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. It outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct</td>
<td>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>watsonx.ai</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>elyza-japanese-llama-2-7b-instruct</td>
<td>ELYZA-japanese-Llama-2-7b は、 Llama2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。</td>
</tr>
<tr>
<td>Flan-t5-xxl</td>
<td>The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</td>
</tr>
<tr>
<td>Flan-ul2</td>
<td>The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</td>
</tr>
<tr>
<td>granite-13b-chat-v2</td>
<td>The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</td>
</tr>
<tr>
<td>granite-13b-instruct-v2</td>
<td>The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</td>
</tr>
<tr>
<td>granite-20b-multilingual</td>
<td>The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</td>
</tr>
<tr>
<td>granite-3-2b-instruct</td>
<td>Granite-3.0-2B-Instruct is a lightweight and open-source 8B parameter model fine tuned from Granite-3.0-8B-Base on a combination of open-source and proprietary instruction data with a permissively licensed. This language model is designed to excel in instruction following tasks such as summarization, problem-solving, text translation, reasoning, code tasks, funcion-calling, and more.</td>
</tr>
<tr>
<td>granite-3-8b-instruct</td>
<td>Granite-3.0-8B-Instruct is a lightweight and open-source 8B parameter model fine tuned from Granite-3.0-8B-Base on a combination of open-source and proprietary instruction data with a permissively licensed. This language model is designed to excel in instruction following tasks such as summarization, problem-solving, text translation, reasoning, code tasks, funcion-calling, and more.</td>
</tr>
<tr>
<td>granite-34b-code-instruct</td>
<td>The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</td>
</tr>
<tr>
<td>granite-7b-lab (Deprecated)</td>
<td>The Granite 7 Billion LAB (granite-7b-lab) model is the chat-focused variant initialized from the pre-trained Granite 7 Billion (granite-7b) model, which is Meta Llama 2 7B architecture trained to 2T tokens.</td>
</tr>
<tr>
<td>granite-8b-japanese</td>
<td>The Granite 8 Billion Japanese model is an instruct variant initialized from the pre-trained Granite Base 8 Billion Japanese model. Pre-training went through 1.0T tokens of English, 0.5T tokens of Japanese, and 0.1T tokens of code. This model is designed to work with Japanese text. IBM Generative AI Large Language Foundation Models are Enterprise-level Multilingual models trained with large volumes of data that has been subjected to intensive pre-processing and careful analysis.</td>
</tr>
<tr>
<td>jais-13b-chat</td>
<td>Jais-13b-chat is Jais-13b fine-tuned over a curated set of 4 million Arabic and 6 million English prompt-response pairs.</td>
</tr>
<tr>
<td>Llama-2-chat 13B (Deprecated)</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Llama-2-chat 70B (Deprecated)</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>llama-3-1-70b-instruct</td>
<td>Llama 3.1 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-1-8b-instruct</td>
<td>Llama 3.1 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-2-11b-vision-instruct</td>
<td>Llama 3.2 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-2-1b-instruct</td>
<td>Llama 3.2 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-2-3b-instruct</td>
<td>Llama 3.2 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-2-90b-vision-instruct</td>
<td>Llama 3.2 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-3-70b-instruct</td>
<td>Llama 3.3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-405b-instruct</td>
<td>llama-3-405b instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-70b-instruct</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-8b-instruct</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama3-llava-next-8b-hf (depricated)</td>
<td>Llama 3-llava Supports image captioning, image-to-text transcription (OCR) including handwriting, data extraction and processing, context Q&amp;A, object identification.</td>
</tr>
<tr>
<td>Mistral-large</td>
<td>The most advanced Mistral AI Large Language model capable of handling any language task including complex multilingual reasoning, text understanding, transformation, and code generation.</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct</td>
<td>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
</tbody>
</table>
</details>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p><strong>LLM choice is available with NeuralSeek’s BYOLLM (bring your own Large Language Model) plan.</strong></p>
<p>LLMs can vary in their capabilities and performances. Some LLM can take up to 30 seconds and longer to generate a full response. Use caution when using in conjunction with a virtual agent platform that imposes a strict timeout.</p>
</div>
<h2 id="configuring_an_llm">Configuring an LLM<a class="headerlink" href="#configuring_an_llm" title="Permanent link"></a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In order to configure an LLM, make sure that you have subscribed to the Bring Your Own LLM (BYOLLM) plan. All other plans will default to NeuralSeek's curated LLM, and this option will not be available.</p>
</div>
<ol>
<li>In NeuralSeek UI, navigate to <code>Configure &gt; LLM Details</code> page, using the top menu.</li>
<li>Click <code>Add an LLM</code> button.</li>
<li>Select the Platform and LLM Selection. (e.g. Platform: Self-Hosted, LLM: Flan-u2)</li>
<li>Click <code>Add</code>.</li>
<li>Enter the <code>LLM API key</code> in the LLM API Key input field.</li>
<li>Review the Enabled Languages (presented as multi-select)</li>
<li>Review the LLM functions available (presented as checkbox)</li>
<li>Click <code>Test</code> button to test whether the API key works.</li>
</ol>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>You must add at least one LLM. If you add multiple, NeuralSeek will load-balance across them for the selected functions that have multiple LLM's. Features that an LLM are not capable of will be unselectable. If you do not provide an LLM for a function, there is no fallback and that function of NeuralSeek will be disabled.</p>
</div>







  
  



  




                
<div id="chat" style="position: fixed; right: 20px; bottom: 20px; width: 450px; display: none; max-height: 60vh; min-height: max(160px, -10px + min(250px, 100vh)); height: calc(-140px + 100vh);"></div>
<div id="chat-toggle-btn" style="position: fixed; right: 20px; bottom: 20px; display: block;"></div>
<div id="chathistory"></div>

              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Ⓒ 2024 NeuralSeek, all rights reserved.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tracking", "navigation.indexes", "navigation.top", "search.suggest", "search.highlight", "search.share", "toc.follow", "content.code.copy"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>